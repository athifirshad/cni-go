Started kubelet: The Kubernetes Node Agent.
Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
I0125 05:46:45.198894     633 server.go:206] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0125 05:46:45.208992     633 server.go:486] "Kubelet version" kubeletVersion="v1.31.1"
I0125 05:46:45.209016     633 server.go:488] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0125 05:46:45.209224     633 server.go:929] "Client rotation is on, will bootstrap in background"
I0125 05:46:45.211894     633 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
I0125 05:46:45.230422     633 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
W0125 05:46:45.234329     633 logging.go:55] [core] [Channel #1 SubChannel #2]grpc: addrConn.createTransport failed to connect to {Addr: "/var/run/containerd/containerd.sock", ServerName: "localhost", }. Err: connection error: desc = "transport: Error while dialing: dial unix /var/run/containerd/containerd.sock: connect: no such file or directory"
E0125 05:46:45.235685     633 run.go:72] "command failed" err="failed to run Kubelet: validate service connection: validate CRI v1 runtime API for endpoint \"unix:///var/run/containerd/containerd.sock\": rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /var/run/containerd/containerd.sock: connect: no such file or directory\""
[0;1;39m[0;1;31m[0;1;39mkubelet.service: Main process exited, code=exited, status=1/FAILURE[0m
[0;1;38;5;185m[0;1;39m[0;1;38;5;185mkubelet.service: Failed with result 'exit-code'.[0m
kubelet.service: Scheduled restart job, restart counter is at 1.
Stopped kubelet: The Kubernetes Node Agent.
Started kubelet: The Kubernetes Node Agent.
Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet's --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
I0125 05:46:55.542988    1888 server.go:206] "--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime"
I0125 05:46:55.549308    1888 server.go:486] "Kubelet version" kubeletVersion="v1.31.1"
I0125 05:46:55.549337    1888 server.go:488] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0125 05:46:55.549565    1888 server.go:929] "Client rotation is on, will bootstrap in background"
I0125 05:46:55.550842    1888 certificate_store.go:130] Loading cert/key pair from "/var/lib/kubelet/pki/kubelet-client-current.pem".
I0125 05:46:55.554311    1888 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
E0125 05:46:55.660800    1888 log.go:32] "RuntimeConfig from runtime service failed" err="rpc error: code = Unimplemented desc = unknown method RuntimeConfig for service runtime.v1.RuntimeService"
I0125 05:46:55.660831    1888 server.go:1403] "CRI implementation should be updated to support RuntimeConfig when KubeletCgroupDriverFromCRI feature gate has been enabled. Falling back to using cgroupDriver from kubelet config."
I0125 05:46:55.666107    1888 server.go:744] "--cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /"
I0125 05:46:55.667138    1888 swap_util.go:113] "Swap is on" /proc/swaps contents="Filename\t\t\t\tType\t\tSize\t\tUsed\t\tPriority"
I0125 05:46:55.667238    1888 container_manager_linux.go:264] "Container manager verified user specified cgroup-root exists" cgroupRoot=[]
I0125 05:46:55.667269    1888 container_manager_linux.go:269] "Creating Container Manager object based on Node Config" nodeConfig={"NodeName":"instance-20241021-1858","RuntimeCgroupsName":"","SystemCgroupsName":"","KubeletCgroupsName":"","KubeletOOMScoreAdj":-999,"ContainerRuntime":"","CgroupsPerQOS":true,"CgroupRoot":"/","CgroupDriver":"systemd","KubeletRootDir":"/var/lib/kubelet","ProtectKernelDefaults":false,"KubeReservedCgroupName":"","SystemReservedCgroupName":"","ReservedSystemCPUs":{},"EnforceNodeAllocatable":{"pods":{}},"KubeReserved":null,"SystemReserved":null,"HardEvictionThresholds":[{"Signal":"memory.available","Operator":"LessThan","Value":{"Quantity":"100Mi","Percentage":0},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.1},"GracePeriod":0,"MinReclaim":null},{"Signal":"nodefs.inodesFree","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null},{"Signal":"imagefs.available","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.15},"GracePeriod":0,"MinReclaim":null},{"Signal":"imagefs.inodesFree","Operator":"LessThan","Value":{"Quantity":null,"Percentage":0.05},"GracePeriod":0,"MinReclaim":null}],"QOSReserved":{},"CPUManagerPolicy":"none","CPUManagerPolicyOptions":null,"TopologyManagerScope":"container","CPUManagerReconcilePeriod":10000000000,"ExperimentalMemoryManagerPolicy":"None","ExperimentalMemoryManagerReservedMemory":null,"PodPidsLimit":-1,"EnforceCPULimits":true,"CPUCFSQuotaPeriod":100000000,"TopologyManagerPolicy":"none","TopologyManagerPolicyOptions":null,"CgroupVersion":2}
I0125 05:46:55.667788    1888 topology_manager.go:138] "Creating topology manager with none policy"
I0125 05:46:55.667804    1888 container_manager_linux.go:300] "Creating device plugin manager"
I0125 05:46:55.667851    1888 state_mem.go:36] "Initialized new in-memory state store"
I0125 05:46:55.669075    1888 kubelet.go:408] "Attempting to sync node with API server"
I0125 05:46:55.669092    1888 kubelet.go:303] "Adding static pod path" path="/etc/kubernetes/manifests"
I0125 05:46:55.669535    1888 kubelet.go:314] "Adding apiserver pod source"
I0125 05:46:55.669549    1888 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
W0125 05:46:55.671286    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://10.0.0.3:6443/api/v1/nodes?fieldSelector=metadata.name%3Dinstance-20241021-1858&limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
W0125 05:46:55.671331    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://10.0.0.3:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:46:55.671348    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://10.0.0.3:6443/api/v1/nodes?fieldSelector=metadata.name%3Dinstance-20241021-1858&limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
E0125 05:46:55.671373    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://10.0.0.3:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
I0125 05:46:55.671874    1888 kuberuntime_manager.go:262] "Container runtime initialized" containerRuntime="containerd" version="1.7.22" apiVersion="v1"
I0125 05:46:55.674138    1888 kubelet.go:837] "Not starting ClusterTrustBundle informer because we are in static kubelet mode"
I0125 05:46:55.717289    1888 server.go:1269] "Started kubelet"
I0125 05:46:55.718659    1888 server.go:163] "Starting to listen" address="0.0.0.0" port=10250
I0125 05:46:55.719863    1888 ratelimit.go:55] "Setting rate limiting for endpoint" service="podresources" qps=100 burstTokens=10
I0125 05:46:55.721736    1888 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
I0125 05:46:55.723241    1888 volume_manager.go:289] "Starting Kubelet Volume Manager"
I0125 05:46:55.722034    1888 dynamic_serving_content.go:135] "Starting controller" name="kubelet-server-cert-files::/var/lib/kubelet/pki/kubelet.crt::/var/lib/kubelet/pki/kubelet.key"
I0125 05:46:55.723619    1888 desired_state_of_world_populator.go:146] "Desired state populator starts to run"
I0125 05:46:55.722491    1888 server.go:460] "Adding debug handlers to kubelet server"
E0125 05:46:55.724703    1888 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"instance-20241021-1858\" not found"
W0125 05:46:55.727282    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://10.0.0.3:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:46:55.727368    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://10.0.0.3:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
E0125 05:46:55.727425    1888 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://10.0.0.3:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/instance-20241021-1858?timeout=10s\": dial tcp 10.0.0.3:6443: connect: connection refused" interval="200ms"
E0125 05:46:55.727489    1888 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://10.0.0.3:6443/api/v1/namespaces/default/events\": dial tcp 10.0.0.3:6443: connect: connection refused" event="&Event{ObjectMeta:{instance-20241021-1858.181dd95ae92de751  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:instance-20241021-1858,UID:instance-20241021-1858,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:instance-20241021-1858,},FirstTimestamp:2025-01-25 05:46:55.717263185 +0000 UTC m=+0.207724081,LastTimestamp:2025-01-25 05:46:55.717263185 +0000 UTC m=+0.207724081,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:instance-20241021-1858,}"
I0125 05:46:55.730139    1888 scope.go:117] "RemoveContainer" containerID="c0346cf7e37a3e25b2fa97ce0b31301c7b8b09003e634d1b072ce045dc5a1f4f"
I0125 05:46:55.731189    1888 factory.go:221] Registration of the systemd container factory successfully
I0125 05:46:55.731327    1888 factory.go:219] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
I0125 05:46:55.732747    1888 factory.go:221] Registration of the containerd container factory successfully
E0125 05:46:55.735736    1888 kubelet.go:1478] "Image garbage collection failed once. Stats initialization may not have completed yet" err="invalid capacity 0 on image filesystem"
I0125 05:46:55.742928    1888 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
I0125 05:46:55.744102    1888 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
I0125 05:46:55.744132    1888 status_manager.go:217] "Starting to sync pod status with apiserver"
I0125 05:46:55.744150    1888 kubelet.go:2321] "Starting kubelet main sync loop"
E0125 05:46:55.744187    1888 kubelet.go:2345] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
W0125 05:46:55.745796    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://10.0.0.3:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:46:55.745850    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://10.0.0.3:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
I0125 05:46:55.764641    1888 cpu_manager.go:214] "Starting CPU manager" policy="none"
I0125 05:46:55.764656    1888 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
I0125 05:46:55.764674    1888 state_mem.go:36] "Initialized new in-memory state store"
I0125 05:46:55.765427    1888 state_mem.go:88] "Updated default CPUSet" cpuSet=""
I0125 05:46:55.765453    1888 state_mem.go:96] "Updated CPUSet assignments" assignments={}
I0125 05:46:55.765474    1888 policy_none.go:49] "None policy: Start"
I0125 05:46:55.766312    1888 memory_manager.go:170] "Starting memorymanager" policy="None"
I0125 05:46:55.766332    1888 state_mem.go:35] "Initializing new in-memory state store"
I0125 05:46:55.767123    1888 state_mem.go:75] "Updated machine memory state"
I0125 05:46:55.825681    1888 server.go:236] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
E0125 05:46:55.825717    1888 kubelet_node_status.go:453] "Error getting the current node from lister" err="node \"instance-20241021-1858\" not found"
E0125 05:46:55.845037    1888 kubelet.go:2345] "Skipping pod synchronization" err="container runtime status check may not have completed yet"
I0125 05:46:55.926089    1888 manager.go:513] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
I0125 05:46:55.926291    1888 eviction_manager.go:189] "Eviction manager: starting control loop"
I0125 05:46:55.926317    1888 container_log_manager.go:189] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
I0125 05:46:55.927652    1888 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
E0125 05:46:55.927954    1888 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://10.0.0.3:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/instance-20241021-1858?timeout=10s\": dial tcp 10.0.0.3:6443: connect: connection refused" interval="400ms"
E0125 05:46:55.928068    1888 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"instance-20241021-1858\" not found"
I0125 05:46:56.027869    1888 kubelet_node_status.go:72] "Attempting to register node" node="instance-20241021-1858"
E0125 05:46:56.028389    1888 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://10.0.0.3:6443/api/v1/nodes\": dial tcp 10.0.0.3:6443: connect: connection refused" node="instance-20241021-1858"
I0125 05:46:56.064437    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="704fd30e6af186cff39aa6fbfd917b568c1774e190bab69020c4a36c6f205269"
I0125 05:46:56.064475    1888 scope.go:117] "RemoveContainer" containerID="e2fd7753f0c8c182b8f4d0e3b5bcefbeb68bdd1b14828c418427ea7b5de62dc6"
I0125 05:46:56.229566    1888 kubelet_node_status.go:72] "Attempting to register node" node="instance-20241021-1858"
E0125 05:46:56.229848    1888 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://10.0.0.3:6443/api/v1/nodes\": dial tcp 10.0.0.3:6443: connect: connection refused" node="instance-20241021-1858"
I0125 05:46:56.293927    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="07e2a59453f37ead0d8f32d18903c1410baf611e1221331cb41ae74ea9ac194f"
I0125 05:46:56.293952    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="7bfb2a949d18194311ba922d3e28e0d529e1a9ec3a1f52dcd645c6aaf55e48ef"
E0125 05:46:56.329091    1888 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://10.0.0.3:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/instance-20241021-1858?timeout=10s\": dial tcp 10.0.0.3:6443: connect: connection refused" interval="800ms"
I0125 05:46:56.358400    1888 scope.go:117] "RemoveContainer" containerID="e275b569ea0827b5469b12b0e15aa8e4b4fc4a8681f1205fd3cfa61e6b3bc767"
W0125 05:46:56.594457    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://10.0.0.3:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:46:56.594530    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://10.0.0.3:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
I0125 05:46:56.631173    1888 kubelet_node_status.go:72] "Attempting to register node" node="instance-20241021-1858"
E0125 05:46:56.631465    1888 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://10.0.0.3:6443/api/v1/nodes\": dial tcp 10.0.0.3:6443: connect: connection refused" node="instance-20241021-1858"
I0125 05:46:56.635331    1888 scope.go:117] "RemoveContainer" containerID="e2fd7753f0c8c182b8f4d0e3b5bcefbeb68bdd1b14828c418427ea7b5de62dc6"
I0125 05:46:56.673553    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="3bcb196760a5020925c3bba5ab47d7a9588cda477e6e54f63804352c25f01d6c"
E0125 05:46:56.674191    1888 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"e2fd7753f0c8c182b8f4d0e3b5bcefbeb68bdd1b14828c418427ea7b5de62dc6\": not found" containerID="e2fd7753f0c8c182b8f4d0e3b5bcefbeb68bdd1b14828c418427ea7b5de62dc6"
I0125 05:46:56.674217    1888 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"containerd","ID":"e2fd7753f0c8c182b8f4d0e3b5bcefbeb68bdd1b14828c418427ea7b5de62dc6"} err="failed to get container status \"e2fd7753f0c8c182b8f4d0e3b5bcefbeb68bdd1b14828c418427ea7b5de62dc6\": rpc error: code = NotFound desc = an error occurred when try to find container \"e2fd7753f0c8c182b8f4d0e3b5bcefbeb68bdd1b14828c418427ea7b5de62dc6\": not found"
I0125 05:46:56.674277    1888 scope.go:117] "RemoveContainer" containerID="e275b569ea0827b5469b12b0e15aa8e4b4fc4a8681f1205fd3cfa61e6b3bc767"
I0125 05:46:56.675556    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c59e7e76a9bfa1c883c41692c52c6c308044935b2fddfcba0e210391fae31199"
E0125 05:46:56.675684    1888 log.go:32] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to set removing state for container \"e275b569ea0827b5469b12b0e15aa8e4b4fc4a8681f1205fd3cfa61e6b3bc767\": container is already in removing state" containerID="e275b569ea0827b5469b12b0e15aa8e4b4fc4a8681f1205fd3cfa61e6b3bc767"
I0125 05:46:56.675710    1888 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"containerd","ID":"e275b569ea0827b5469b12b0e15aa8e4b4fc4a8681f1205fd3cfa61e6b3bc767"} err="rpc error: code = Unknown desc = failed to set removing state for container \"e275b569ea0827b5469b12b0e15aa8e4b4fc4a8681f1205fd3cfa61e6b3bc767\": container is already in removing state"
I0125 05:46:56.675725    1888 scope.go:117] "RemoveContainer" containerID="e275b569ea0827b5469b12b0e15aa8e4b4fc4a8681f1205fd3cfa61e6b3bc767"
E0125 05:46:56.676989    1888 log.go:32] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to set removing state for container \"e275b569ea0827b5469b12b0e15aa8e4b4fc4a8681f1205fd3cfa61e6b3bc767\": container is already in removing state" containerID="e275b569ea0827b5469b12b0e15aa8e4b4fc4a8681f1205fd3cfa61e6b3bc767"
I0125 05:46:56.677016    1888 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"containerd","ID":"e275b569ea0827b5469b12b0e15aa8e4b4fc4a8681f1205fd3cfa61e6b3bc767"} err="rpc error: code = Unknown desc = failed to set removing state for container \"e275b569ea0827b5469b12b0e15aa8e4b4fc4a8681f1205fd3cfa61e6b3bc767\": container is already in removing state"
W0125 05:46:56.753429    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://10.0.0.3:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:46:56.753493    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://10.0.0.3:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
I0125 05:46:56.875491    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a667e77233ab68e6a2971e7b555e2ff309dc8db7bc5c62009c4ec59f1838a74b"
I0125 05:46:56.875516    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b193e9bdbc88019b2867e25d0e42aa03bdefa9e1441490d8326f8a14b92677a6"
I0125 05:46:56.875516    1888 scope.go:117] "RemoveContainer" containerID="7a63fa113989fa94f68865902cfed1d6cb93001d70ee34c6b366814eaec4290e"
I0125 05:46:57.020340    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c597534755ac27cb8f82181b92d08f97513caa9af6641f158c1829787ffb00ec"
I0125 05:46:57.020365    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="06ef74dfce766ac95c8541aff823fd50f741cb26c295d13e40a4500d2a6e5719"
W0125 05:46:57.021270    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://10.0.0.3:6443/api/v1/nodes?fieldSelector=metadata.name%3Dinstance-20241021-1858&limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:46:57.021344    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://10.0.0.3:6443/api/v1/nodes?fieldSelector=metadata.name%3Dinstance-20241021-1858&limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
I0125 05:46:57.026451    1888 scope.go:117] "RemoveContainer" containerID="47dd54487af6a01f662ca882ecbf9a3af9181066c7914d3314bbecfbeb0cf106"
I0125 05:46:57.088652    1888 reconciler.go:26] "Reconciler: start to sync state"
I0125 05:46:57.088720    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/b021170e30b8bc44bfba6fd3a7063bbd-etcd-certs\") pod \"etcd-instance-20241021-1858\" (UID: \"b021170e30b8bc44bfba6fd3a7063bbd\") " pod="kube-system/etcd-instance-20241021-1858"
I0125 05:46:57.088743    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/085daefa216daa24a6db9856711bd8af-etc-ca-certificates\") pod \"kube-apiserver-instance-20241021-1858\" (UID: \"085daefa216daa24a6db9856711bd8af\") " pod="kube-system/kube-apiserver-instance-20241021-1858"
I0125 05:46:57.088767    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/085daefa216daa24a6db9856711bd8af-usr-local-share-ca-certificates\") pod \"kube-apiserver-instance-20241021-1858\" (UID: \"085daefa216daa24a6db9856711bd8af\") " pod="kube-system/kube-apiserver-instance-20241021-1858"
I0125 05:46:57.088787    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/146b5dfd1e4eac5f2397ff63540d91b4-etc-ca-certificates\") pod \"kube-controller-manager-instance-20241021-1858\" (UID: \"146b5dfd1e4eac5f2397ff63540d91b4\") " pod="kube-system/kube-controller-manager-instance-20241021-1858"
I0125 05:46:57.088803    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/146b5dfd1e4eac5f2397ff63540d91b4-usr-share-ca-certificates\") pod \"kube-controller-manager-instance-20241021-1858\" (UID: \"146b5dfd1e4eac5f2397ff63540d91b4\") " pod="kube-system/kube-controller-manager-instance-20241021-1858"
I0125 05:46:57.088825    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/085daefa216daa24a6db9856711bd8af-k8s-certs\") pod \"kube-apiserver-instance-20241021-1858\" (UID: \"085daefa216daa24a6db9856711bd8af\") " pod="kube-system/kube-apiserver-instance-20241021-1858"
I0125 05:46:57.088876    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/146b5dfd1e4eac5f2397ff63540d91b4-k8s-certs\") pod \"kube-controller-manager-instance-20241021-1858\" (UID: \"146b5dfd1e4eac5f2397ff63540d91b4\") " pod="kube-system/kube-controller-manager-instance-20241021-1858"
I0125 05:46:57.088925    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/146b5dfd1e4eac5f2397ff63540d91b4-kubeconfig\") pod \"kube-controller-manager-instance-20241021-1858\" (UID: \"146b5dfd1e4eac5f2397ff63540d91b4\") " pod="kube-system/kube-controller-manager-instance-20241021-1858"
I0125 05:46:57.088953    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/146b5dfd1e4eac5f2397ff63540d91b4-usr-local-share-ca-certificates\") pod \"kube-controller-manager-instance-20241021-1858\" (UID: \"146b5dfd1e4eac5f2397ff63540d91b4\") " pod="kube-system/kube-controller-manager-instance-20241021-1858"
I0125 05:46:57.088985    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/146b5dfd1e4eac5f2397ff63540d91b4-ca-certs\") pod \"kube-controller-manager-instance-20241021-1858\" (UID: \"146b5dfd1e4eac5f2397ff63540d91b4\") " pod="kube-system/kube-controller-manager-instance-20241021-1858"
I0125 05:46:57.089005    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/146b5dfd1e4eac5f2397ff63540d91b4-flexvolume-dir\") pod \"kube-controller-manager-instance-20241021-1858\" (UID: \"146b5dfd1e4eac5f2397ff63540d91b4\") " pod="kube-system/kube-controller-manager-instance-20241021-1858"
I0125 05:46:57.089024    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/f775ce4f009b0711646d6336f28cf27b-kubeconfig\") pod \"kube-scheduler-instance-20241021-1858\" (UID: \"f775ce4f009b0711646d6336f28cf27b\") " pod="kube-system/kube-scheduler-instance-20241021-1858"
I0125 05:46:57.089041    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/b021170e30b8bc44bfba6fd3a7063bbd-etcd-data\") pod \"etcd-instance-20241021-1858\" (UID: \"b021170e30b8bc44bfba6fd3a7063bbd\") " pod="kube-system/etcd-instance-20241021-1858"
I0125 05:46:57.089059    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/085daefa216daa24a6db9856711bd8af-ca-certs\") pod \"kube-apiserver-instance-20241021-1858\" (UID: \"085daefa216daa24a6db9856711bd8af\") " pod="kube-system/kube-apiserver-instance-20241021-1858"
I0125 05:46:57.089075    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/085daefa216daa24a6db9856711bd8af-usr-share-ca-certificates\") pod \"kube-apiserver-instance-20241021-1858\" (UID: \"085daefa216daa24a6db9856711bd8af\") " pod="kube-system/kube-apiserver-instance-20241021-1858"
E0125 05:46:57.129569    1888 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://10.0.0.3:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/instance-20241021-1858?timeout=10s\": dial tcp 10.0.0.3:6443: connect: connection refused" interval="1.6s"
W0125 05:46:57.160665    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://10.0.0.3:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:46:57.160733    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://10.0.0.3:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
I0125 05:46:57.195213    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="f9c7a5a7d6b23e4dbf56ea65b913ba5320b050cce4d7f6cd0429caf5d035f218"
I0125 05:46:57.196239    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="a04300f9fc5be552699974da127cc1d5cf1a8965f26889bc9c1777666b123265"
I0125 05:46:57.294065    1888 scope.go:117] "RemoveContainer" containerID="7a63fa113989fa94f68865902cfed1d6cb93001d70ee34c6b366814eaec4290e"
E0125 05:46:57.295447    1888 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"7a63fa113989fa94f68865902cfed1d6cb93001d70ee34c6b366814eaec4290e\": not found" containerID="7a63fa113989fa94f68865902cfed1d6cb93001d70ee34c6b366814eaec4290e"
I0125 05:46:57.295484    1888 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"containerd","ID":"7a63fa113989fa94f68865902cfed1d6cb93001d70ee34c6b366814eaec4290e"} err="failed to get container status \"7a63fa113989fa94f68865902cfed1d6cb93001d70ee34c6b366814eaec4290e\": rpc error: code = NotFound desc = an error occurred when try to find container \"7a63fa113989fa94f68865902cfed1d6cb93001d70ee34c6b366814eaec4290e\": not found"
I0125 05:46:57.295506    1888 scope.go:117] "RemoveContainer" containerID="ce5b3d17d74ef1bc5cbb5c55a53f6faec079ae83ecc1a8533312d4cc44ca71dd"
I0125 05:46:57.313265    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c0346cf7e37a3e25b2fa97ce0b31301c7b8b09003e634d1b072ce045dc5a1f4f"
I0125 05:46:57.313295    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="48920e86155740c2e6de74a91442f037ef65bac9f39249c178446e071bfa38dd"
I0125 05:46:57.433378    1888 kubelet_node_status.go:72] "Attempting to register node" node="instance-20241021-1858"
E0125 05:46:57.433663    1888 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://10.0.0.3:6443/api/v1/nodes\": dial tcp 10.0.0.3:6443: connect: connection refused" node="instance-20241021-1858"
I0125 05:46:57.439445    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2e54c1d22387a847c463a6f0fe936d6f0c0facefe913bb7fabe172e37d319cf5"
I0125 05:46:57.509761    1888 scope.go:117] "RemoveContainer" containerID="fc8e2da19e94f36baf398214d9becc9e6f7110f920459640c626a983bdd3e747"
E0125 05:46:57.595209    1888 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"47dd54487af6a01f662ca882ecbf9a3af9181066c7914d3314bbecfbeb0cf106\": not found" containerID="47dd54487af6a01f662ca882ecbf9a3af9181066c7914d3314bbecfbeb0cf106"
I0125 05:46:57.595325    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="47dd54487af6a01f662ca882ecbf9a3af9181066c7914d3314bbecfbeb0cf106"
I0125 05:46:57.595340    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="545c6d8fb8794d387939c5bd15d15af2434f6844864153be0136dd328875e1a5"
I0125 05:46:57.595349    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b7c4019e336a73a4d7d6bb745addfebef2ef3a073f58f5c5b4afe1af0d313ef5"
I0125 05:46:57.722168    1888 scope.go:117] "RemoveContainer" containerID="ce5b3d17d74ef1bc5cbb5c55a53f6faec079ae83ecc1a8533312d4cc44ca71dd"
E0125 05:46:57.722588    1888 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"ce5b3d17d74ef1bc5cbb5c55a53f6faec079ae83ecc1a8533312d4cc44ca71dd\": not found" containerID="ce5b3d17d74ef1bc5cbb5c55a53f6faec079ae83ecc1a8533312d4cc44ca71dd"
I0125 05:46:57.722620    1888 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"containerd","ID":"ce5b3d17d74ef1bc5cbb5c55a53f6faec079ae83ecc1a8533312d4cc44ca71dd"} err="failed to get container status \"ce5b3d17d74ef1bc5cbb5c55a53f6faec079ae83ecc1a8533312d4cc44ca71dd\": rpc error: code = NotFound desc = an error occurred when try to find container \"ce5b3d17d74ef1bc5cbb5c55a53f6faec079ae83ecc1a8533312d4cc44ca71dd\": not found"
I0125 05:46:57.722643    1888 scope.go:117] "RemoveContainer" containerID="fc8e2da19e94f36baf398214d9becc9e6f7110f920459640c626a983bdd3e747"
E0125 05:46:57.724425    1888 log.go:32] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to set removing state for container \"fc8e2da19e94f36baf398214d9becc9e6f7110f920459640c626a983bdd3e747\": container is already in removing state" containerID="fc8e2da19e94f36baf398214d9becc9e6f7110f920459640c626a983bdd3e747"
I0125 05:46:57.724498    1888 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"containerd","ID":"fc8e2da19e94f36baf398214d9becc9e6f7110f920459640c626a983bdd3e747"} err="rpc error: code = Unknown desc = failed to set removing state for container \"fc8e2da19e94f36baf398214d9becc9e6f7110f920459640c626a983bdd3e747\": container is already in removing state"
I0125 05:46:57.724515    1888 scope.go:117] "RemoveContainer" containerID="fc8e2da19e94f36baf398214d9becc9e6f7110f920459640c626a983bdd3e747"
E0125 05:46:57.725708    1888 log.go:32] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to set removing state for container \"fc8e2da19e94f36baf398214d9becc9e6f7110f920459640c626a983bdd3e747\": container is already in removing state" containerID="fc8e2da19e94f36baf398214d9becc9e6f7110f920459640c626a983bdd3e747"
I0125 05:46:57.725738    1888 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"containerd","ID":"fc8e2da19e94f36baf398214d9becc9e6f7110f920459640c626a983bdd3e747"} err="rpc error: code = Unknown desc = failed to set removing state for container \"fc8e2da19e94f36baf398214d9becc9e6f7110f920459640c626a983bdd3e747\": container is already in removing state"
I0125 05:46:57.758906    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b00bf5cae2f4d89f7cf68f97b0b3ccb4eb98568e4f797a920d30a1d844417d21"
I0125 05:46:57.758936    1888 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2021b9fa728e64f03b1094a4d52aa6a8cae1a17866443f092cc3273e2ee0a096"
I0125 05:46:57.758949    1888 scope.go:117] "RemoveContainer" containerID="2eb11d79d7fccb834e5b50885ba2611079b29752abfa78bb347653760a12df8d"
I0125 05:46:58.162643    1888 scope.go:117] "RemoveContainer" containerID="2eb11d79d7fccb834e5b50885ba2611079b29752abfa78bb347653760a12df8d"
E0125 05:46:58.163070    1888 log.go:32] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"2eb11d79d7fccb834e5b50885ba2611079b29752abfa78bb347653760a12df8d\": not found" containerID="2eb11d79d7fccb834e5b50885ba2611079b29752abfa78bb347653760a12df8d"
I0125 05:46:58.163107    1888 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"containerd","ID":"2eb11d79d7fccb834e5b50885ba2611079b29752abfa78bb347653760a12df8d"} err="failed to get container status \"2eb11d79d7fccb834e5b50885ba2611079b29752abfa78bb347653760a12df8d\": rpc error: code = NotFound desc = an error occurred when try to find container \"2eb11d79d7fccb834e5b50885ba2611079b29752abfa78bb347653760a12df8d\": not found"
I0125 05:46:58.387098    1888 scope.go:117] "RemoveContainer" containerID="2eb11d79d7fccb834e5b50885ba2611079b29752abfa78bb347653760a12df8d"
E0125 05:46:58.389666    1888 kuberuntime_gc.go:150] "Failed to remove container" err="failed to get container status \"2eb11d79d7fccb834e5b50885ba2611079b29752abfa78bb347653760a12df8d\": rpc error: code = NotFound desc = an error occurred when try to find container \"2eb11d79d7fccb834e5b50885ba2611079b29752abfa78bb347653760a12df8d\": not found" containerID="2eb11d79d7fccb834e5b50885ba2611079b29752abfa78bb347653760a12df8d"
I0125 05:46:58.389704    1888 scope.go:117] "RemoveContainer" containerID="7a63fa113989fa94f68865902cfed1d6cb93001d70ee34c6b366814eaec4290e"
E0125 05:46:58.390095    1888 kuberuntime_gc.go:150] "Failed to remove container" err="failed to get container status \"7a63fa113989fa94f68865902cfed1d6cb93001d70ee34c6b366814eaec4290e\": rpc error: code = NotFound desc = an error occurred when try to find container \"7a63fa113989fa94f68865902cfed1d6cb93001d70ee34c6b366814eaec4290e\": not found" containerID="7a63fa113989fa94f68865902cfed1d6cb93001d70ee34c6b366814eaec4290e"
I0125 05:46:58.390117    1888 scope.go:117] "RemoveContainer" containerID="ce5b3d17d74ef1bc5cbb5c55a53f6faec079ae83ecc1a8533312d4cc44ca71dd"
E0125 05:46:58.390557    1888 kuberuntime_gc.go:150] "Failed to remove container" err="failed to get container status \"ce5b3d17d74ef1bc5cbb5c55a53f6faec079ae83ecc1a8533312d4cc44ca71dd\": rpc error: code = NotFound desc = an error occurred when try to find container \"ce5b3d17d74ef1bc5cbb5c55a53f6faec079ae83ecc1a8533312d4cc44ca71dd\": not found" containerID="ce5b3d17d74ef1bc5cbb5c55a53f6faec079ae83ecc1a8533312d4cc44ca71dd"
I0125 05:46:58.390577    1888 scope.go:117] "RemoveContainer" containerID="e2fd7753f0c8c182b8f4d0e3b5bcefbeb68bdd1b14828c418427ea7b5de62dc6"
E0125 05:46:58.393422    1888 kuberuntime_gc.go:150] "Failed to remove container" err="failed to get container status \"e2fd7753f0c8c182b8f4d0e3b5bcefbeb68bdd1b14828c418427ea7b5de62dc6\": rpc error: code = NotFound desc = an error occurred when try to find container \"e2fd7753f0c8c182b8f4d0e3b5bcefbeb68bdd1b14828c418427ea7b5de62dc6\": not found" containerID="e2fd7753f0c8c182b8f4d0e3b5bcefbeb68bdd1b14828c418427ea7b5de62dc6"
W0125 05:46:58.655043    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://10.0.0.3:6443/api/v1/nodes?fieldSelector=metadata.name%3Dinstance-20241021-1858&limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:46:58.655086    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://10.0.0.3:6443/api/v1/nodes?fieldSelector=metadata.name%3Dinstance-20241021-1858&limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
E0125 05:46:58.729918    1888 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://10.0.0.3:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/instance-20241021-1858?timeout=10s\": dial tcp 10.0.0.3:6443: connect: connection refused" interval="3.2s"
I0125 05:46:59.035695    1888 kubelet_node_status.go:72] "Attempting to register node" node="instance-20241021-1858"
E0125 05:46:59.036021    1888 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://10.0.0.3:6443/api/v1/nodes\": dial tcp 10.0.0.3:6443: connect: connection refused" node="instance-20241021-1858"
W0125 05:46:59.096680    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://10.0.0.3:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:46:59.096754    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://10.0.0.3:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
W0125 05:46:59.149327    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://10.0.0.3:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:46:59.149397    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://10.0.0.3:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
W0125 05:47:00.331390    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://10.0.0.3:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:47:00.331466    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://10.0.0.3:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
E0125 05:47:01.930842    1888 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://10.0.0.3:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/instance-20241021-1858?timeout=10s\": dial tcp 10.0.0.3:6443: connect: connection refused" interval="6.4s"
I0125 05:47:02.237192    1888 kubelet_node_status.go:72] "Attempting to register node" node="instance-20241021-1858"
E0125 05:47:02.237473    1888 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://10.0.0.3:6443/api/v1/nodes\": dial tcp 10.0.0.3:6443: connect: connection refused" node="instance-20241021-1858"
W0125 05:47:02.481171    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://10.0.0.3:6443/api/v1/nodes?fieldSelector=metadata.name%3Dinstance-20241021-1858&limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:47:02.481241    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://10.0.0.3:6443/api/v1/nodes?fieldSelector=metadata.name%3Dinstance-20241021-1858&limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
W0125 05:47:02.834473    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://10.0.0.3:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:47:02.834520    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://10.0.0.3:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
W0125 05:47:03.992504    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://10.0.0.3:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:47:03.992570    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://10.0.0.3:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
W0125 05:47:04.367434    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://10.0.0.3:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:47:04.367517    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://10.0.0.3:6443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
E0125 05:47:05.048674    1888 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://10.0.0.3:6443/api/v1/namespaces/default/events\": dial tcp 10.0.0.3:6443: connect: connection refused" event="&Event{ObjectMeta:{instance-20241021-1858.181dd95ae92de751  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:instance-20241021-1858,UID:instance-20241021-1858,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:instance-20241021-1858,},FirstTimestamp:2025-01-25 05:46:55.717263185 +0000 UTC m=+0.207724081,LastTimestamp:2025-01-25 05:46:55.717263185 +0000 UTC m=+0.207724081,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:instance-20241021-1858,}"
E0125 05:47:05.928790    1888 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"instance-20241021-1858\" not found"
E0125 05:47:08.331843    1888 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://10.0.0.3:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/instance-20241021-1858?timeout=10s\": dial tcp 10.0.0.3:6443: connect: connection refused" interval="7s"
I0125 05:47:08.639303    1888 kubelet_node_status.go:72] "Attempting to register node" node="instance-20241021-1858"
E0125 05:47:08.639593    1888 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://10.0.0.3:6443/api/v1/nodes\": dial tcp 10.0.0.3:6443: connect: connection refused" node="instance-20241021-1858"
W0125 05:47:10.316732    1888 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://10.0.0.3:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 10.0.0.3:6443: connect: connection refused
E0125 05:47:10.316808    1888 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://10.0.0.3:6443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 10.0.0.3:6443: connect: connection refused" logger="UnhandledError"
I0125 05:47:15.642194    1888 kubelet_node_status.go:72] "Attempting to register node" node="instance-20241021-1858"
E0125 05:47:15.929614    1888 eviction_manager.go:285] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"instance-20241021-1858\" not found"
I0125 05:47:18.196832    1888 kubelet_node_status.go:111] "Node was previously registered" node="instance-20241021-1858"
I0125 05:47:18.196923    1888 kubelet_node_status.go:75] "Successfully registered node" node="instance-20241021-1858"
I0125 05:47:18.196947    1888 kuberuntime_manager.go:1635] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
I0125 05:47:18.197772    1888 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
I0125 05:47:18.682297    1888 apiserver.go:52] "Watching apiserver"
I0125 05:47:18.724213    1888 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
I0125 05:47:18.801453    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/9b542f42-ad8f-444f-aa1a-80feb8c359e7-xtables-lock\") pod \"kube-proxy-l4zdf\" (UID: \"9b542f42-ad8f-444f-aa1a-80feb8c359e7\") " pod="kube-system/kube-proxy-l4zdf"
I0125 05:47:18.801513    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-socket-dir\" (UniqueName: \"kubernetes.io/host-path/7736800d-1d4d-4585-91fe-abf22cd096bf-cni-socket-dir\") pod \"cni-manager-bd9b96c5d-gztx4\" (UID: \"7736800d-1d4d-4585-91fe-abf22cd096bf\") " pod="kube-system/cni-manager-bd9b96c5d-gztx4"
I0125 05:47:18.801542    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/b4fe4a37-466a-486d-a5d2-45d8ed95d3dd-lib-modules\") pod \"cni-daemon-59sgf\" (UID: \"b4fe4a37-466a-486d-a5d2-45d8ed95d3dd\") " pod="kube-system/cni-daemon-59sgf"
I0125 05:47:18.801558    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-socket-dir\" (UniqueName: \"kubernetes.io/host-path/b4fe4a37-466a-486d-a5d2-45d8ed95d3dd-cni-socket-dir\") pod \"cni-daemon-59sgf\" (UID: \"b4fe4a37-466a-486d-a5d2-45d8ed95d3dd\") " pod="kube-system/cni-daemon-59sgf"
I0125 05:47:18.801607    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"run\" (UniqueName: \"kubernetes.io/host-path/b4fe4a37-466a-486d-a5d2-45d8ed95d3dd-run\") pod \"cni-daemon-59sgf\" (UID: \"b4fe4a37-466a-486d-a5d2-45d8ed95d3dd\") " pod="kube-system/cni-daemon-59sgf"
I0125 05:47:18.801635    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/9b542f42-ad8f-444f-aa1a-80feb8c359e7-lib-modules\") pod \"kube-proxy-l4zdf\" (UID: \"9b542f42-ad8f-444f-aa1a-80feb8c359e7\") " pod="kube-system/kube-proxy-l4zdf"
I0125 05:47:18.801661    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni\" (UniqueName: \"kubernetes.io/host-path/b4fe4a37-466a-486d-a5d2-45d8ed95d3dd-cni\") pod \"cni-daemon-59sgf\" (UID: \"b4fe4a37-466a-486d-a5d2-45d8ed95d3dd\") " pod="kube-system/cni-daemon-59sgf"
I0125 05:47:18.801677    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"run\" (UniqueName: \"kubernetes.io/host-path/7736800d-1d4d-4585-91fe-abf22cd096bf-run\") pod \"cni-manager-bd9b96c5d-gztx4\" (UID: \"7736800d-1d4d-4585-91fe-abf22cd096bf\") " pod="kube-system/cni-manager-bd9b96c5d-gztx4"
I0125 05:47:18.801694    1888 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-net-dir\" (UniqueName: \"kubernetes.io/host-path/b4fe4a37-466a-486d-a5d2-45d8ed95d3dd-cni-net-dir\") pod \"cni-daemon-59sgf\" (UID: \"b4fe4a37-466a-486d-a5d2-45d8ed95d3dd\") " pod="kube-system/cni-daemon-59sgf"
E0125 05:47:22.010859    1888 log.go:32] "StartContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"/usr/local/bin/cni-manager\": stat /usr/local/bin/cni-manager: no such file or directory: unknown" containerID="d4f80dcc80cf28c2cd04b25c7adbd7b9bd0020864256a1a13a7c684b2e905208"
E0125 05:47:22.011197    1888 kuberuntime_manager.go:1274] "Unhandled Error" err="container &Container{Name:cni-manager,Image:localhost:5000/cni-daemon:latest,Command:[/usr/local/bin/cni-manager],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:run,ReadOnly:false,MountPath:/var/run,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-m74tw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd): RunContainerError: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"/usr/local/bin/cni-manager\": stat /usr/local/bin/cni-manager: no such file or directory: unknown" logger="UnhandledError"
E0125 05:47:22.012727    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with RunContainerError: \"failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \\\"/usr/local/bin/cni-manager\\\": stat /usr/local/bin/cni-manager: no such file or directory: unknown\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:47:22.922029    1888 scope.go:117] "RemoveContainer" containerID="f04dec2ff661e0cb019bbadf938ee66a572f733d0588773bdf105ddb580276d8"
I0125 05:47:22.922039    1888 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
I0125 05:47:22.922259    1888 scope.go:117] "RemoveContainer" containerID="d4f80dcc80cf28c2cd04b25c7adbd7b9bd0020864256a1a13a7c684b2e905208"
E0125 05:47:22.922377    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 10s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:47:23.928721    1888 scope.go:117] "RemoveContainer" containerID="d4f80dcc80cf28c2cd04b25c7adbd7b9bd0020864256a1a13a7c684b2e905208"
E0125 05:47:23.928849    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 10s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
W0125 05:47:25.041527    1888 manager.go:1169] Failed to process watch event {EventType:0 Name:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb4fe4a37_466a_486d_a5d2_45d8ed95d3dd.slice/cri-containerd-d4f80dcc80cf28c2cd04b25c7adbd7b9bd0020864256a1a13a7c684b2e905208.scope WatchSource:0}: task d4f80dcc80cf28c2cd04b25c7adbd7b9bd0020864256a1a13a7c684b2e905208 not found: not found
I0125 05:47:35.745284    1888 scope.go:117] "RemoveContainer" containerID="d4f80dcc80cf28c2cd04b25c7adbd7b9bd0020864256a1a13a7c684b2e905208"
E0125 05:47:35.861938    1888 log.go:32] "StartContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"/usr/local/bin/cni-manager\": stat /usr/local/bin/cni-manager: no such file or directory: unknown" containerID="46270e0453449cbdf37f044e0a5941e18d7be3475919f104688f8b450df4787e"
E0125 05:47:35.862084    1888 kuberuntime_manager.go:1274] "Unhandled Error" err="container &Container{Name:cni-manager,Image:localhost:5000/cni-daemon:latest,Command:[/usr/local/bin/cni-manager],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:run,ReadOnly:false,MountPath:/var/run,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-m74tw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd): RunContainerError: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"/usr/local/bin/cni-manager\": stat /usr/local/bin/cni-manager: no such file or directory: unknown" logger="UnhandledError"
E0125 05:47:35.863546    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with RunContainerError: \"failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \\\"/usr/local/bin/cni-manager\\\": stat /usr/local/bin/cni-manager: no such file or directory: unknown\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:47:35.954398    1888 scope.go:117] "RemoveContainer" containerID="d4f80dcc80cf28c2cd04b25c7adbd7b9bd0020864256a1a13a7c684b2e905208"
I0125 05:47:35.954740    1888 scope.go:117] "RemoveContainer" containerID="46270e0453449cbdf37f044e0a5941e18d7be3475919f104688f8b450df4787e"
E0125 05:47:35.954905    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
W0125 05:47:38.954257    1888 manager.go:1169] Failed to process watch event {EventType:0 Name:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb4fe4a37_466a_486d_a5d2_45d8ed95d3dd.slice/cri-containerd-46270e0453449cbdf37f044e0a5941e18d7be3475919f104688f8b450df4787e.scope WatchSource:0}: task 46270e0453449cbdf37f044e0a5941e18d7be3475919f104688f8b450df4787e not found: not found
I0125 05:47:48.745618    1888 scope.go:117] "RemoveContainer" containerID="46270e0453449cbdf37f044e0a5941e18d7be3475919f104688f8b450df4787e"
E0125 05:47:48.745750    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:47:49.048183    1888 scope.go:117] "RemoveContainer" containerID="46270e0453449cbdf37f044e0a5941e18d7be3475919f104688f8b450df4787e"
E0125 05:47:49.066628    1888 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = FailedPrecondition desc = failed to exec in container: failed to start exec \"23e6d09b623982e3ffe5cbbc7c75d8c329d220600be3ba505a276eed50ba398a\": container a4e87597b2b355c7655ed3f90798a8b2fda153fb43cc63c0fa92c3b91aa94027 init process is not running: failed precondition" containerID="a4e87597b2b355c7655ed3f90798a8b2fda153fb43cc63c0fa92c3b91aa94027" cmd=["/bin/sh","-c","test -e /var/run/cni/daemon.sock"]
E0125 05:47:49.076342    1888 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = failed to exec in container: failed to create exec \"5c85ca385cf53b233490d5803a01ee738e9543a00327ded7a53ca47a37ae1bb1\": cannot exec in a stopped state: unknown" containerID="a4e87597b2b355c7655ed3f90798a8b2fda153fb43cc63c0fa92c3b91aa94027" cmd=["/bin/sh","-c","test -e /var/run/cni/daemon.sock"]
E0125 05:47:49.086553    1888 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = NotFound desc = failed to exec in container: failed to create exec \"5f5ba3e0db0a26886349902b01462d48c82131c1763b982364f57b02311d1b79\": container not created: not found" containerID="a4e87597b2b355c7655ed3f90798a8b2fda153fb43cc63c0fa92c3b91aa94027" cmd=["/bin/sh","-c","test -e /var/run/cni/daemon.sock"]
I0125 05:47:49.994399    1888 scope.go:117] "RemoveContainer" containerID="3883d3beeaf86013f83eff2f137d3aa3c407a4c60534cbb563d55b5b11ee9518"
E0125 05:47:50.374756    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:47:51.000537    1888 scope.go:117] "RemoveContainer" containerID="46270e0453449cbdf37f044e0a5941e18d7be3475919f104688f8b450df4787e"
E0125 05:47:51.000794    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:47:52.002514    1888 scope.go:117] "RemoveContainer" containerID="46270e0453449cbdf37f044e0a5941e18d7be3475919f104688f8b450df4787e"
E0125 05:47:52.002649    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:47:53.004521    1888 scope.go:117] "RemoveContainer" containerID="46270e0453449cbdf37f044e0a5941e18d7be3475919f104688f8b450df4787e"
E0125 05:47:53.004652    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 20s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:48:07.744965    1888 scope.go:117] "RemoveContainer" containerID="46270e0453449cbdf37f044e0a5941e18d7be3475919f104688f8b450df4787e"
E0125 05:48:07.854907    1888 log.go:32] "StartContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"/usr/local/bin/cni-manager\": stat /usr/local/bin/cni-manager: no such file or directory: unknown" containerID="dbfbf06e4b8663e338025fd2b85941842b2a3c85fabb6c86b69bb25f80a279c5"
E0125 05:48:07.855034    1888 kuberuntime_manager.go:1274] "Unhandled Error" err="container &Container{Name:cni-manager,Image:localhost:5000/cni-daemon:latest,Command:[/usr/local/bin/cni-manager],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:NODE_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:run,ReadOnly:false,MountPath:/var/run,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-m74tw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd): RunContainerError: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"/usr/local/bin/cni-manager\": stat /usr/local/bin/cni-manager: no such file or directory: unknown" logger="UnhandledError"
E0125 05:48:07.856192    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with RunContainerError: \"failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \\\"/usr/local/bin/cni-manager\\\": stat /usr/local/bin/cni-manager: no such file or directory: unknown\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:48:08.037566    1888 scope.go:117] "RemoveContainer" containerID="46270e0453449cbdf37f044e0a5941e18d7be3475919f104688f8b450df4787e"
I0125 05:48:08.037921    1888 scope.go:117] "RemoveContainer" containerID="dbfbf06e4b8663e338025fd2b85941842b2a3c85fabb6c86b69bb25f80a279c5"
E0125 05:48:08.038064    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 40s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
W0125 05:48:10.946621    1888 manager.go:1169] Failed to process watch event {EventType:0 Name:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podb4fe4a37_466a_486d_a5d2_45d8ed95d3dd.slice/cri-containerd-dbfbf06e4b8663e338025fd2b85941842b2a3c85fabb6c86b69bb25f80a279c5.scope WatchSource:0}: task dbfbf06e4b8663e338025fd2b85941842b2a3c85fabb6c86b69bb25f80a279c5 not found: not found
I0125 05:48:20.744911    1888 scope.go:117] "RemoveContainer" containerID="dbfbf06e4b8663e338025fd2b85941842b2a3c85fabb6c86b69bb25f80a279c5"
E0125 05:48:20.745075    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 40s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:48:29.051488    1888 scope.go:117] "RemoveContainer" containerID="dbfbf06e4b8663e338025fd2b85941842b2a3c85fabb6c86b69bb25f80a279c5"
E0125 05:48:29.066862    1888 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = FailedPrecondition desc = failed to exec in container: failed to start exec \"391af8c877cf720f82eea154d511551377b415725276979eee4a47a9be69ba15\": container 4c9e04afa474116b3b75932cedcdb1198ad42d6b2de1a4e3e8632db697296a31 init process is not running: failed precondition" containerID="4c9e04afa474116b3b75932cedcdb1198ad42d6b2de1a4e3e8632db697296a31" cmd=["/bin/sh","-c","test -e /var/run/cni/daemon.sock"]
E0125 05:48:29.073164    1888 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = failed to exec in container: failed to create exec \"63d6002478e6dd96f8696802d19a07bf1528b298308458286f5beda8dc827eaf\": cannot exec in a stopped state: unknown" containerID="4c9e04afa474116b3b75932cedcdb1198ad42d6b2de1a4e3e8632db697296a31" cmd=["/bin/sh","-c","test -e /var/run/cni/daemon.sock"]
E0125 05:48:29.092146    1888 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = NotFound desc = failed to exec in container: failed to create exec \"be971317aede76e879ced5e7546e257e5e34e1e114e56767db496da554b7ae2a\": task 4c9e04afa474116b3b75932cedcdb1198ad42d6b2de1a4e3e8632db697296a31 not found: not found" containerID="4c9e04afa474116b3b75932cedcdb1198ad42d6b2de1a4e3e8632db697296a31" cmd=["/bin/sh","-c","test -e /var/run/cni/daemon.sock"]
E0125 05:48:29.193497    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 40s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:48:30.085868    1888 scope.go:117] "RemoveContainer" containerID="a4e87597b2b355c7655ed3f90798a8b2fda153fb43cc63c0fa92c3b91aa94027"
I0125 05:48:30.086256    1888 scope.go:117] "RemoveContainer" containerID="dbfbf06e4b8663e338025fd2b85941842b2a3c85fabb6c86b69bb25f80a279c5"
E0125 05:48:30.086370    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 40s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
I0125 05:48:31.092083    1888 scope.go:117] "RemoveContainer" containerID="dbfbf06e4b8663e338025fd2b85941842b2a3c85fabb6c86b69bb25f80a279c5"
E0125 05:48:31.092643    1888 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cni-manager\" with CrashLoopBackOff: \"back-off 40s restarting failed container=cni-manager pod=cni-daemon-59sgf_kube-system(b4fe4a37-466a-486d-a5d2-45d8ed95d3dd)\"" pod="kube-system/cni-daemon-59sgf" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd"
E0125 05:48:41.147653    1888 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = failed to exec in container: failed to create exec \"e8e64562bedd182f6b035cbace880d83ae0abe41922e2de7a010063400ff5627\": cannot exec in a stopped state: unknown" containerID="d52c8bcc90701845feb6431f41f404fa5d87f3701dbac52735e0ccfd16900953" cmd=["/bin/sh","-c","test -e /var/run/cni/daemon.sock"]
E0125 05:48:41.189688    1888 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = failed to exec in container: failed to create exec \"7515378f51969c1d871be1720970626ed808c2d57b5a77870acf5fe09e7511f4\": cannot exec in a deleted state: unknown" containerID="d52c8bcc90701845feb6431f41f404fa5d87f3701dbac52735e0ccfd16900953" cmd=["/bin/sh","-c","test -e /var/run/cni/daemon.sock"]
E0125 05:48:41.194003    1888 log.go:32] "ExecSync cmd from runtime service failed" err="rpc error: code = NotFound desc = failed to exec in container: failed to load task: no running task found: task d52c8bcc90701845feb6431f41f404fa5d87f3701dbac52735e0ccfd16900953 not found: not found" containerID="d52c8bcc90701845feb6431f41f404fa5d87f3701dbac52735e0ccfd16900953" cmd=["/bin/sh","-c","test -e /var/run/cni/daemon.sock"]
E0125 05:48:41.217269    1888 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd" containerName="cni-manager"
E0125 05:48:41.217303    1888 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd" containerName="cni-daemon"
E0125 05:48:41.217326    1888 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="b4fe4a37-466a-486d-a5d2-45d8ed95d3dd" containerName="cni-daemon"
E0125 05:48:41.217333    1888 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="dde9feb6-e330-42b2-a977-016